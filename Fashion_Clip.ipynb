{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7da7c3-44de-4384-b5dd-8b9af51e24b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0652310-bfe7-44e5-8b77-fe20f22afb48",
   "metadata": {},
   "source": [
    "## Installing required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845fe187-e6c4-44b2-8182-a7d487fb126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision open-clip-torch matplotlib pandas tqdm scikit-learn\n",
    "# also get open-clip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976f8c7-9d3f-4b34-93f8-f8bc47ceef51",
   "metadata": {},
   "source": [
    "## Building the Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6820991-cce9-44ae-9a64-6ec2dc636a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class FashionCLIPDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_folder, preprocess, max_samples=None):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df = df.dropna(subset=['productDisplayName'])\n",
    "        if max_samples:\n",
    "            df = df.sample(n=max_samples, random_state=42)\n",
    "        self.df = df\n",
    "        self.image_folder = image_folder\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.image_folder, f\"{row['id']}.jpg\")\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except:\n",
    "            return None\n",
    "        image = self.preprocess(image)\n",
    "        text = row['productDisplayName']\n",
    "        return image, text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a578cc-c209-4266-8362-d7223bc75422",
   "metadata": {},
   "source": [
    "## Training + Testing Loop with Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e9950-af33-4a8d-833b-b3a0278a1a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2B-s34B-b79K')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "model.to(device)\n",
    "\n",
    "# Split dataset\n",
    "csv_path = \"fashion-dataset/styles.csv\"\n",
    "image_folder = \"fashion-dataset/images\"\n",
    "train_df, val_df = train_test_split(pd.read_csv(csv_path).dropna(subset=['productDisplayName']), test_size=0.1, random_state=42)\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "val_df.to_csv(\"val.csv\", index=False)\n",
    "\n",
    "train_dataset = FashionCLIPDataset(\"train.csv\", image_folder, preprocess, max_samples=5000)\n",
    "val_dataset = FashionCLIPDataset(\"val.csv\", image_folder, preprocess, max_samples=1000)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses, val_losses, accuracies = [], [], []\n",
    "\n",
    "for epoch in range(5):  # adjust as needed\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        if batch is None: continue\n",
    "        images, texts = batch\n",
    "        images = images.to(device)\n",
    "        texts = tokenizer(texts).to(device)\n",
    "\n",
    "        image_features = model.encode_image(images)\n",
    "        text_features = model.encode_text(texts)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logits_per_image = image_features @ text_features.T\n",
    "        logits_per_text = logits_per_image.T\n",
    "\n",
    "        labels = torch.arange(len(images), device=device)\n",
    "        loss = (loss_fn(logits_per_image, labels) + loss_fn(logits_per_text, labels)) / 2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = logits_per_image.argmax(dim=1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += len(images)\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_acc = total_correct / total_samples\n",
    "    train_losses.append(avg_train_loss)\n",
    "    accuracies.append(train_acc)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, texts in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = images.to(device)\n",
    "            texts = tokenizer(texts).to(device)\n",
    "\n",
    "            image_features = model.encode_image(images)\n",
    "            text_features = model.encode_text(texts)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            logits = image_features @ text_features.T\n",
    "            labels = torch.arange(len(images), device=device)\n",
    "            val_loss += loss_fn(logits, labels).item()\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f} | Val Loss={val_losses[-1]:.4f} | Accuracy={train_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c6b0da-4b21-41df-a00d-8ab7749c7263",
   "metadata": {},
   "source": [
    "## Plot Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc0f7f0-f1ff-452d-9d16-9125ca154606",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(accuracies, label=\"Accuracy\", color=\"green\")\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61392a07-66a4-45c6-bb90-284a586febbf",
   "metadata": {},
   "source": [
    "## Text → Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed41bcc-85c4-46da-a78f-5f457ed709df",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"red floral dress\"\n",
    "text_features = model.encode_text(tokenizer([query]).to(device))\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Compare to all validation images\n",
    "image_features = []\n",
    "paths = []\n",
    "for img, txt in val_dataset:\n",
    "    if img is None: continue\n",
    "    with torch.no_grad():\n",
    "        emb = model.encode_image(img.unsqueeze(0).to(device))\n",
    "        emb /= emb.norm(dim=-1, keepdim=True)\n",
    "    image_features.append(emb.cpu())\n",
    "image_features = torch.vstack(image_features)\n",
    "scores = (image_features @ text_features.cpu().T).squeeze()\n",
    "best_idx = scores.argmax().item()\n",
    "print(f\"Best match: {val_dataset.df.iloc[best_idx]['productDisplayName']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72308076-e78f-4a23-9a12-380c30cd1329",
   "metadata": {},
   "source": [
    "## Image → Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079694c-3a89-4555-9d70-f87a689ab78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image = val_dataset[0][0].unsqueeze(0).to(device)\n",
    "query_emb = model.encode_image(query_image)\n",
    "query_emb /= query_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "sims = (image_features @ query_emb.cpu().T).squeeze()\n",
    "top5 = sims.topk(5).indices\n",
    "print(val_dataset.df.iloc[top5]['productDisplayName'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f6335-ee97-4967-a134-59dd37de6717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923025c0-a5ae-45ac-8a23-07dd6f905fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de192bb-34fe-415e-891e-f4a5c38c6260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
